---
title: "stochastic process thinks"
collection: thinks
type: "maths"
permalink: /thinks/2024-10-math-1
date: 2024-10-28
---

# Math thinks(you should refer to stoiastic process files)

[markdown上实现latex公式1](https://www.zybuluo.com/codeep/note/163962)

[markdown上实现latex公式2](https://www.cnblogs.com/nowgood/p/Latexstart.html)

### set vs outcome(集合中的元素) vs event(is also a set)
目前正在学习随机过程，sample space `S` is  a set, sigma-algebra is a collection of subsets of the sample space, probability measure is a function that assigns probabilities to events in the sigma-algebra.

### RV
RV X(s) is a function that assigns a value to an outcome s ∈ S⇒ Think of RVs as measurements associated with an experiment

![rv的实例](../images/math1.png)

### markov inequality

![markov不等式证明](../images/math2.png)

### random process definition

A random process is a collection (or ensemble) of RVs{X(s, t)}that are functions of a real variable, namely timet where s∈S (sample space) and t∈T (parameter set or index set).

The set of possible values of any individual member of the random process called state space. Any individual member itself is called a sample function or a realisation of the process.

### almost sure convergence vs convergence in probability(see lecture 4.pdf)

- Almost sure convergence: A sequence of RVs {Xn} converges almost surely to a RV X if P({s: limn→∞Xn(s) = X(s)}) = 1

- Convergence in probability: A sequence of RVs {Xn} converges in probability to a RV X if for any ε > 0, limn→∞P(|Xn − X| > ε) = 0

### law of total probability is important in probability theory

this law is playing a key role in the derivation of the markov's chain.

### trasient state vs recurrent state

fi 是指从状态i开始，在有限时间内回到状态i（第一次）的概率，如果fi=1，那么状态i是recurrent state，否则是transient state。
要与常规的定义(无穷为recurrent)区分开。

![实例](../images/math3.png)

### limiting distribution

the limiting behavior of Markov chains as time $n \to \infty$.In particular, under suitable easy-to-check conditions, we will see that a Markov chain possessesa limiting probability distribution, $\pi = (\pi_{j})_{j\in S}$ and that the chain, if started off initially withsuch a distribution will be a stationary stochastic process.

### notice

写latex时，如果要在行内插入公式，使用`$`符号，如果要在行间插入公式，使用`$$`符号。以及最好空格一下，不然会出现一些问题。

### prove to be a mc 

[a good example](https://math.stackexchange.com/questions/358678/prove-markov-chain-by-definition)

[another example](https://math.stackexchange.com/questions/1357575/is-x-n-n-geq-0-a-markov-chain?rq=1)

[same question](https://math.stackexchange.com/questions/1014619/how-to-transform-a-process-into-a-markov-chain)

Cartesian product in set theory makes one dimension to two dimensions.

[a solution](https://math.stackexchange.com/questions/1358906/if-pr-has-all-positive-entries-then-so-does-pn)

$P_{ij}^{(n)}$ is the probability of going from state i to state j in n steps.

$$
P_{ij}^{(n)} = \sum_{k\in S}P_{ik}^{(n-1)}P_{kj}
$$

$f_{i}$ 表示从state i开始，再次回到 i 的概率（不计时间）。如果 $f_{i} = 1$，则称状态 i 是recurrent state，否则是transient state。
也可以表述为回到状态i的次数是无穷的。

### communication class

if i can go to j and j can go to i($i \leftrightarrow j$ ), then i and j are in the same communication class(transient or recurrent property).

### remark of finite chains

1. In a finite-state MC all recurrent states are positive recurrent.

2. a finite mc cannot consist only of transient states.

3. [1956.pdf](https://sites.pitt.edu/~super7/19011-20001/19561.pdf) will answer many of your quenstions(like relation $f_{ij}^{(n)}$ and $p_{ij}^{(n)}$(more common))!!!

### limiting distribution

π is called a stationary (or equilibrium) distribution of the Markov chain if it satisfies
$$
\pi = \pi P
$$
($\pi$ is a row vector)

or in terms of components
$$
\pi_{j} = \sum_{i\in S}\pi_{i}P_{ij},\forall j\in S
$$
so if tranpose the equation, we can get
$$
P^{T}\pi^{T} = \pi^{T}(with \sum_{j\in S}\pi_{j} = 1)
$$

### ergodicity of a mc
$$
\lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^{n}P_{ij}^{(k)} = \pi_{j}
$$
no proof,but can also refer to the [lec4LimitingProb.pdf](https://www.sjsu.edu/faculty/guangliang.chen/Math263/lec4LimitingProb.pdf)

$$
T = \sum_{n=1}^{l}I_{n}
$$
where $I_{n}$ is the indicator function of the event that the chain is in state j at time n.
T is the number of visits to state j in the first l steps of the chain.

The proportion of visits to state j in l steps is
$$
\frac{T}{l} = \frac{1}{l}\sum_{n=1}^{l}I_{n}
$$

### counting process

some definitions:

1. jumps at random times with steps of random sizes 

2. non decreasing and right continuous

3. $N(t)$ is the number of jumps up to time 

4. $N(t) - N(s)$ is the number of jumps in the interval $(s,t]$

poisson process has three definitions.

$$
P(N(t) = n) = P(N(t) < n+1) - P(N(t) < n)
$$

### exponential distribution

$\lambda$ is the rate of the exponential distribution,or the inverse of $\lambda$ is the mean time of event happening once(单位相同).

another interpretation is that average of $$\lambda t$$ events by time t.

a tip :P(T>s+t) = P(T>s)P(T>t) 从公式可快速证明。

a mathematically proof of the tip:[mathematica foundation](https://math.stackexchange.com/questions/293371/how-do-i-prove-that-fxfy-fxy-implies-that-fx-ecx-assuming-f-is)

### guarantee time of exponential distribution
can refer to 19501.pdf (guarantee time)
$$
E(T|T>t) = t + E(T)
$$

### poisson distribution

$$
P(N(t)=n) = e^{-\lambda t}\frac{(-\lambda t)^{n}}{n!},E(N(t))=\lambda t.
$$

N(t) and $S_{n}$(time of nth event happening-arrival time) have different distribution (poisson vs gamma) ,but have $P(S_{n}>t)=P(N(t)<n)$.

indepent increments mean that the events in `disjoint` intervals are independent

a important note: $ N(t)=n \equiv (S_{n}<t,S_{n+1}>t)$ !!!!

### sum of iid exponential random variables

$$
S_{n} = \sum_{i=1}^{n}X_{i} \sim Gamma(n,\lambda),E(S_{n}) = \frac{n}{\lambda}
f_{T}(t) = \lambda e^{-\lambda t} \frac{(\lambda t)^{n-1}}{(n-1)!}
$$

[first problem](https://math.stackexchange.com/questions/2725476/conditional-expectation-of-a-poisson-process-given-an-overlapping-interval?rq=1)

[second problem](https://math.stackexchange.com/questions/2843370/conditional-expectation-of-a-poisson-process-with-a-given-parameter?rq=1)

[third problem](https://stats.stackexchange.com/questions/369204/conditional-expectation-of-poisson-process-given-number-of-events)

[an interesting phenomenon](https://stats.stackexchange.com/questions/412037/conditional-distribution-of-arrival-times-in-poisson-process?rq=1)

### conditional distribution of arrival times in Poisson process

the answer is the joint pdf of the order statistics of n iid uniform random variables on (0,T)

An interesting property of Poisson processes is that each event can be considered as "placed" independently and uniformly at a given time t
in [0,T] (just like rain drops falling uniformly over the length of a board of length T ). In other words,
if there are n events ${\tau_{i}}^{n}_{i=1}$, we have τi∼Unif(0,T) for all i.

[fourth problem](https://math.stackexchange.com/questions/3037464/poisson-process-expectation-of-time-of-an-event-given-number-of-events-until-tha)

[fifth problem](https://math.stackexchange.com/questions/3789122/conditional-expected-value-of-a-poisson-process)























