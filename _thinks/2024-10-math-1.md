---
title: "stochastic process thinks"
collection: thinks
type: "maths"
permalink: /thinks/2024-10-math-1
date: 2024-10-28
---

# Math thinks(you should refer to stoiastic process files)

[markdown上实现latex公式1](https://www.zybuluo.com/codeep/note/163962)

[markdown上实现latex公式2](https://www.cnblogs.com/nowgood/p/Latexstart.html)

### set vs outcome(集合中的元素) vs event(is also a set)
目前正在学习随机过程，sample space `S` is  a set, sigma-algebra is a collection of subsets of the sample space, probability measure is a function that assigns probabilities to events in the sigma-algebra.

### RV
RV X(s) is a function that assigns a value to an outcome s ∈ S⇒ Think of RVs as measurements associated with an experiment

![rv的实例](../images/math1.png)

### markov inequality

![markov不等式证明](../images/math2.png)

### random process definition

A random process is a collection (or ensemble) of RVs{X(s, t)}that are functions of a real variable, namely timet where s∈S (sample space) and t∈T (parameter set or index set).

The set of possible values of any individual member of the random process called state space. Any individual member itself is called a sample function or a realisation of the process.

### almost sure convergence vs convergence in probability(see lecture 4.pdf)

- Almost sure convergence: A sequence of RVs {Xn} converges almost surely to a RV X if P({s: limn→∞Xn(s) = X(s)}) = 1

- Convergence in probability: A sequence of RVs {Xn} converges in probability to a RV X if for any ε > 0, limn→∞P(|Xn − X| > ε) = 0

### law of total probability is important in probability theory

this law is playing a key role in the derivation of the markov's chain.

### trasient state vs recurrent state

fi 是指从状态i开始，在有限时间内回到状态i（第一次）的概率，如果fi=1，那么状态i是recurrent state，否则是transient state。
要与常规的定义(无穷为recurrent)区分开。

![实例](../images/math3.png)

### limiting distribution

the limiting behavior of Markov chains as time $n \to \infty$.In particular, under suitable easy-to-check conditions, we will see that a Markov chain possessesa limiting probability distribution, $\pi = (\pi_{j})_{j\in S}$ and that the chain, if started off initially withsuch a distribution will be a stationary stochastic process.

### notice

写latex时，如果要在行内插入公式，使用`$`符号，如果要在行间插入公式，使用`$$`符号。以及最好空格一下，不然会出现一些问题。

### prove to be a mc 

[a good example](https://math.stackexchange.com/questions/358678/prove-markov-chain-by-definition)

[another example](https://math.stackexchange.com/questions/1357575/is-x-n-n-geq-0-a-markov-chain?rq=1)

[same question](https://math.stackexchange.com/questions/1014619/how-to-transform-a-process-into-a-markov-chain)

Cartesian product in set theory makes one dimension to two dimensions.

[a solution](https://math.stackexchange.com/questions/1358906/if-pr-has-all-positive-entries-then-so-does-pn)

$P_{ij}^{(n)}$ is the probability of going from state i to state j in n steps.

$$
P_{ij}^{(n)} = \sum_{k\in S}P_{ik}^{(n-1)}P_{kj}
$$

$f_{i}$ 表示从state i开始，再次回到 i 的概率（不计时间）。如果 $f_{i} = 1$，则称状态 i 是recurrent state，否则是transient state。
也可以表述为回到状态i的次数是无穷的。

### communication class

if i can go to j and j can go to i($i \leftrightarrow j$ ), then i and j are in the same communication class(transient or recurrent property).

### remark of finite chains

1. In a finite-state MC all recurrent states are positive recurrent.

2. a finite mc cannot consist only of transient states.

3. [1956.pdf](https://sites.pitt.edu/~super7/19011-20001/19561.pdf) will answer many of your quenstions(like relation $f_{ij}^{(n)}$ and $p_{ij}^{(n)}$(more common))!!!

### limiting distribution

π is called a stationary (or equilibrium) distribution of the Markov chain if it satisfies
$$
\pi = \pi P
$$
($\pi$ is a row vector)

or in terms of components
$$
\pi_{j} = \sum_{i\in S}\pi_{i}P_{ij},\forall j\in S
$$
so if tranpose the equation, we can get
$$
P^{T}\pi^{T} = \pi^{T}(with \sum_{j\in S}\pi_{j} = 1)
$$

### ergodicity of a mc
$$
\lim_{n\to\infty}\frac{1}{n}\sum_{k=1}^{n}P_{ij}^{(k)} = \pi_{j}
$$
no proof,but can also refer to the [lec4LimitingProb.pdf](https://www.sjsu.edu/faculty/guangliang.chen/Math263/lec4LimitingProb.pdf)

$$
T = \sum_{n=1}^{l}I_{n}
$$
where $I_{n}$ is the indicator function of the event that the chain is in state j at time n.
T is the number of visits to state j in the first l steps of the chain.

The proportion of visits to state j in l steps is
$$
\frac{T}{l} = \frac{1}{l}\sum_{n=1}^{l}I_{n}
$$



















